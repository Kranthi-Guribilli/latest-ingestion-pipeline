#!/usr/bin/python3

import asyncio
import json
import aio_pika
import hashlib
import concurrent.futures
from rejson import Client, Path

rg_dict = {}


def key_generator(data):
    print('> Inside key_generator******')
    print('> data(key_generator)---- ' + data.decode())
    
    data_dict = json.loads(data.decode())
    data_redis_q = {}
    default = '_d'

    # extract resource-group from the data packet

    res_id = data_dict['id']
    rg = res_id.split('/')[3]
    sha_id = hashlib.sha1(res_id.encode())
    print("> RG from data---- " + rg)
    print("> rg_dict---- " + str(rg_dict))
    print("> SHA1[res_id]---- " + sha_id.hexdigest())
    # Check if _rg is present in rg_dict{}

    if rg in rg_dict.keys():
        print('> RG is present.')
        # encode SHA1 of resource-id

        attribute = rg_dict[rg]
        path_param = "." + sha_id.hexdigest() + '_' + data_dict[attribute]
    else:
        print('> RG is not present.')
        path_param = "." + sha_id.hexdigest() + default
    
    data_redis_q['key'] = rg;
    data_redis_q['path_param'] = path_param
    data_redis_q['data'] = data_dict
    print('> data_redis_q---- ' + json.dumps(data_redis_q))
    return json.dumps(data_redis_q).encode()


def insert_into_redis(client, data):
    data_dict = json.dumps(data.decode())
    print('> redis_key: ' + data_dict['key'] + ' redis_path_param: ' + data_dict['path_param'])
    # this needs to be tested
    some_test_data_id = data_dict['data']['id']
    client.jsonset(data_dict['key'], Path.rootPath(), json.dumps({}))
    client.jsonset(data_dict['key'], data_dict['path_param'], data_dict['data'])

    if (json.loads(client.jsonget(data_dict['key'], data_dict['path_param']))['id']
            == some_test_data_id):
        return 'success'
    else:
        return 'failed'


async def main_loop(loop):
    # load the dictionary using the config file
    with open('attribute_list.json') as rg_json_f:
        rg_dict = json.load(rg_json_f)
    print('> RG is loaded: ' + str(rg_dict))

    # Connect to Redis client
    try:
        # Connect to Redis
        redis_client = Client(host='redis-server', port=6379, decode_responses=True)

        # Connect to RabbitMQ
        # Can use connection pool instead of two separate clients
        rmq_pub_client = await aio_pika.connect_robust(host='tasks.rabbitmq',port=5672,login='redis-user',
                                                       password='uv)aqcY]qSvARi74', virtualhost='IUDX',loop=loop
        )
    except ConnectionError as e:
        print('> Connection Failed!')
        # return redis_client, rmq_pub_client


    rmq_q_name = "redis-latest"
    redis_q_name = "redis-ingestion-queue"
    routing_key = "latest-data-for-redis"
    rmq_latest_exchange = "redis-latest-ex"
    async with rmq_pub_client:
        channel = await rmq_pub_client.channel()
        queue_rmq = await channel.declare_queue(rmq_q_name, durable=True)
        queue_redis = await channel.declare_queue(redis_q_name, durable=True)

        # declare an exchange to push {data} generated by Key generator for consumption by redis_queue
        latest_exchange = await channel.declare_exchange(rmq_latest_exchange, "direct", durable=True)

        # bind this latest_exchange to redis_queue
        await queue_redis.bind(latest_exchange, routing_key)

        async with queue_rmq.iterator() as queue_iter_1:
            async for message1 in queue_iter_1:
                async with message1.process():
                    # call key_generator(message1.body())[using Futures implementation]
                    with concurrent.futures.ProcessPoolExecutor() as pool:
                        result = await loop.run_in_executor(pool, key_generator(message1.body))
                        print("> result from SHA1_generator---- " + result)
                        # insert result to latest_exchange
                        latest_exchange.publish(latest_exchange, result, routing_key=routing_key, mandatory=True)

        # async with rmq_sub_client:
        # channel = await rmq_sub_client.channel()

        # pull from the queue_redis and call insert_into_redis [using Futures implementation]
        # redis-ingestion-queue has to be bound to the aforementioned exchange
        async with queue_redis.iterator() as queue_iter_2:
            async for message2 in queue_iter_2:
                async with message2.process():

                    # call insert_into_redis(message2.body())[using Futures implementation]
                    with concurrent.futures.ThreadPoolExecutor() as pool:
                        result = await loop.run_in_executor(pool, insert_into_redis(redis_client, message2.body))

                        # Surround with a try catch block?
                        # remove print and use logs instead
                        if result is not None and result == 'success':
                            print('Insertion was successful.')
                        else:
                            print('Insertion failed!')


if __name__ == '__main__':
    print('> Running v0.0.1 Redis Client.')

    # write the asyncio part
    loop = asyncio.get_event_loop()
    # Get both redis and rmq connections here as a tuple to close connections
    loop.create_task(main_loop(loop))
    loop.run_forever()
    # add a catch for exception handling during connection errors
    #   try:
    #       loop.run_forever()
    #   finally:
    #       # close connections
    #       loop.run_until_complete(rmq_con.close())
    #       loop.run_until_complete(redis_con.close())
